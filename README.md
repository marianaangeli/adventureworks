# Data Engineering Pipeline: SQL Server & REST API Integration

## üìñ Descri√ß√£o

Este projeto consiste no desenvolvimento de um pipeline de dados (ETL) robusto para a ingest√£o, processamento e armazenamento de dados provenientes de duas fontes distintas: um banco de dados relacional SQL Server e uma API REST de transa√ß√µes comerciais.

A solu√ß√£o foi projetada para garantir a escalabilidade e a rastreabilidade das informa√ß√µes. O pipeline realiza a extra√ß√£o dos dados brutos (camada Bronze/Raw), utilizando Python para automa√ß√£o e Apache Spark (PySpark) para o processamento distribu√≠do. A arquitetura foca em boas pr√°ticas de engenharia, como o gerenciamento de vari√°veis de ambiente para seguran√ßa e um sistema centralizado de logs para monitoramento de falhas.

O projeto √© compat√≠vel com ambientes de nuvem, como o Databricks, e utiliza conex√µes seguras via JDBC e autentica√ß√£o b√°sica para garantir a integridade da comunica√ß√£o entre os sistemas.

## ‚öôÔ∏è Setup do Projeto

### 1. Clonar o Reposit√≥rio

```bash
git clone https://github.com/seu-usuario/seu-repositorio.git
```

### 2. Configurar Vari√°veis de Ambiente

O projeto utiliza um arquivo `.env` para gerenciar credenciais de forma segura.

- Copie o arquivo de exemplo:
  ```bash
  cp .env.example .env
  ```
- Preencha o arquivo `.env` com as credenciais de acesso (Host, Usu√°rio, Senha, Banco de Dados).

**Nota:** O arquivo `.env` est√° configurado no `.gitignore` para evitar a exposi√ß√£o de dados sens√≠veis.

### 3. Instalar Depend√™ncias

Certifique-se de ter o Python 3.x instalado. Recomenda-se o uso de um ambiente virtual:

```bash
python -m venv venv
source venv/bin/activate  # No Windows: venv\Scripts\activate
pip install -r requirements.txt
```

## üìÅ Estrutura do Projeto

A organiza√ß√£o segue padr√µes de modulariza√ß√£o para facilitar a manuten√ß√£o:

- **config/**: Centraliza o carregamento das vari√°veis de ambiente e configura√ß√µes globais.
- **data/**: Diret√≥rios locais destinados ao armazenamento tempor√°rio de dados (Raw/Processed).
- **logs/**: Armazena os registros de execu√ß√£o do pipeline (INFO, ERROR, DEBUG).
- **notebooks/**: Scripts de valida√ß√£o, testes de conectividade JDBC/API e an√°lise explorat√≥ria.
- **src/**: Core do projeto, contendo os scripts de extra√ß√£o, classes de logging e utilit√°rios.

## üìè Conven√ß√µes e Padr√µes

- **Estilo de C√≥digo:** Segue as diretrizes do Google Python Style Guide.
- **Tratamento de Exce√ß√µes:** Blocos de captura de erro estruturados para lidar com falhas de conex√£o e timeout.
- **Logging Profissional:** Implementa√ß√£o de um logger customizado que registra eventos tanto no console quanto em arquivos f√≠sicos com timestamp.

## üöÄ Roadmap de Desenvolvimento

- [x] Configura√ß√£o do ambiente e gerenciamento de depend√™ncias.
- [x] Valida√ß√£o de conectividade com Banco de Dados via JDBC/Spark.
- [x] Teste e integra√ß√£o com API REST (Basic Auth e tratamento de JSON).
- [ ] Implementa√ß√£o da rotina de extra√ß√£o completa das tabelas de neg√≥cio.
- [ ] Padroniza√ß√£o e limpeza dos dados para a camada Silver.
- [ ] Orquestra√ß√£o do pipeline para execu√ß√µes agendadas.
